{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80b40ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>brand</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>store_location</th>\n",
       "      <th>base_price</th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>promotion_type</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>season</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>avg_units_sold_30d</th>\n",
       "      <th>avg_customers_30d</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>profit_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P100901</td>\n",
       "      <td>L'Oreal Sports Item</td>\n",
       "      <td>Sports</td>\n",
       "      <td>L'Oreal</td>\n",
       "      <td>S010</td>\n",
       "      <td>Amazon Store 10</td>\n",
       "      <td>US-East</td>\n",
       "      <td>211.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>No Promotion</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>Tue</td>\n",
       "      <td>Spring</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>0.332</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P100424</td>\n",
       "      <td>Samsung Electronics Item</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>S035</td>\n",
       "      <td>Amazon Store 35</td>\n",
       "      <td>US-West</td>\n",
       "      <td>231.01</td>\n",
       "      <td>0.35</td>\n",
       "      <td>Flash Sale</td>\n",
       "      <td>339</td>\n",
       "      <td>12</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Fall</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P100014</td>\n",
       "      <td>Adidas Home Item</td>\n",
       "      <td>Home</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>S029</td>\n",
       "      <td>Amazon Store 29</td>\n",
       "      <td>US-East</td>\n",
       "      <td>147.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>No Promotion</td>\n",
       "      <td>193</td>\n",
       "      <td>7</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.238</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P100848</td>\n",
       "      <td>HP Grocery Item</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>HP</td>\n",
       "      <td>S049</td>\n",
       "      <td>Amazon Store 49</td>\n",
       "      <td>US-East</td>\n",
       "      <td>23.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>Clearance</td>\n",
       "      <td>283</td>\n",
       "      <td>10</td>\n",
       "      <td>Mon</td>\n",
       "      <td>Fall</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P100122</td>\n",
       "      <td>AmazonBasics Grocery Item</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>AmazonBasics</td>\n",
       "      <td>S002</td>\n",
       "      <td>Amazon Store 2</td>\n",
       "      <td>US-East</td>\n",
       "      <td>29.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>336</td>\n",
       "      <td>12</td>\n",
       "      <td>Mon</td>\n",
       "      <td>Fall</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>0.247</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id               product_name     category         brand store_id  \\\n",
       "0    P100901        L'Oreal Sports Item       Sports       L'Oreal     S010   \n",
       "1    P100424   Samsung Electronics Item  Electronics       Samsung     S035   \n",
       "2    P100014           Adidas Home Item         Home        Adidas     S029   \n",
       "3    P100848            HP Grocery Item      Grocery            HP     S049   \n",
       "4    P100122  AmazonBasics Grocery Item      Grocery  AmazonBasics     S002   \n",
       "\n",
       "        store_name store_location  base_price  discount_rate promotion_type  \\\n",
       "0  Amazon Store 10        US-East      211.96           0.00   No Promotion   \n",
       "1  Amazon Store 35        US-West      231.01           0.35     Flash Sale   \n",
       "2  Amazon Store 29        US-East      147.61           0.00   No Promotion   \n",
       "3  Amazon Store 49        US-East       23.40           0.58      Clearance   \n",
       "4   Amazon Store 2        US-East       29.66           0.11        Holiday   \n",
       "\n",
       "   day_of_year  month day_of_week  season  is_holiday  avg_units_sold_30d  \\\n",
       "0          168      6         Tue  Spring           0                  14   \n",
       "1          339     12         Wed    Fall           0                  25   \n",
       "2          193      7         Sat  Summer           0                   8   \n",
       "3          283     10         Mon    Fall           0                  23   \n",
       "4          336     12         Mon    Fall           0                  32   \n",
       "\n",
       "   avg_customers_30d  profit_margin profit_class  \n",
       "0                 33          0.332         high  \n",
       "1                 35         -0.136         loss  \n",
       "2                 11          0.238       medium  \n",
       "3                 39         -0.500         loss  \n",
       "4                 45          0.247       medium  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/retail_profit_margin_dataset_30k.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f25416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing promotion_type\n",
    "df[\"promotion_type\"] = df[\"promotion_type\"].fillna(\"No Promotion\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c09f940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profit_class_quantile\n",
       "high      10245\n",
       "medium     9858\n",
       "loss       5202\n",
       "low        4695\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = df[\"profit_margin\"].quantile(0.33)\n",
    "q2 = df[\"profit_margin\"].quantile(0.66)\n",
    "\n",
    "def profit_bucket_quantile(x):\n",
    "    if x < 0:\n",
    "        return \"loss\"\n",
    "    elif x < q1:\n",
    "        return \"low\"\n",
    "    elif x < q2:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "    \n",
    "df[\"profit_class_quantile\"] = df[\"profit_margin\"].apply(profit_bucket_quantile)\n",
    "\n",
    "df[\"profit_class_quantile\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492358dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (21000, 15) Validation: (4500, 15) Test: (4500, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=[\"profit_margin\",\"profit_class\", \"profit_class_quantile\", \"product_name\",\n",
    "    \"store_name\"])\n",
    "y = df[\"profit_class_quantile\"]  # use string labels for now\n",
    "\n",
    "# First split: train + temp (val+test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: validation + test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Print sizes\n",
    "print(\"Train:\", X_train.shape, \"Validation:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e96b95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30000.000000\n",
       "mean         0.190109\n",
       "std          0.228891\n",
       "min         -0.500000\n",
       "25%          0.090000\n",
       "50%          0.245000\n",
       "75%          0.351000\n",
       "max          0.500000\n",
       "Name: profit_margin, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"profit_margin\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26532a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit_class_quantile\n",
      "high      7172\n",
      "loss      6901\n",
      "low       6901\n",
      "medium    6901\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Combine X and y for resampling\n",
    "train_df = X_train.copy()\n",
    "train_df[\"profit_class_quantile\"] = y_train.values\n",
    "\n",
    "# Separate classes\n",
    "df_loss   = train_df[train_df[\"profit_class_quantile\"] == \"loss\"]\n",
    "df_low    = train_df[train_df[\"profit_class_quantile\"] == \"low\"]\n",
    "df_medium = train_df[train_df[\"profit_class_quantile\"] == \"medium\"]\n",
    "df_high   = train_df[train_df[\"profit_class_quantile\"] == \"high\"]\n",
    "\n",
    "# Target size = size of medium class\n",
    "target_size = len(df_medium)\n",
    "\n",
    "# Upsample minority classes\n",
    "df_loss_up = resample(\n",
    "    df_loss,\n",
    "    replace=True,\n",
    "    n_samples=target_size,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_low_up = resample(\n",
    "    df_low,\n",
    "    replace=True,\n",
    "    n_samples=target_size,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine back\n",
    "train_balanced = pd.concat(\n",
    "    [df_loss_up, df_low_up, df_medium, df_high],\n",
    "    axis=0\n",
    ").sample(frac=1, random_state=42)  # shuffle\n",
    "\n",
    "# Split back to X / y\n",
    "X_train_bal = train_balanced.drop(columns=[\"profit_class_quantile\"])\n",
    "y_train_bal = train_balanced[\"profit_class_quantile\"]\n",
    "\n",
    "# Check distribution\n",
    "print(y_train_bal.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c35566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Ready for model training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simple feature engineering\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Price features\n",
    "    df['actual_price'] = df['base_price'] * (1 - df['discount_rate'])\n",
    "    df['discount_amount'] = df['base_price'] * df['discount_rate']\n",
    "    \n",
    "    # Sales features\n",
    "    df['units_per_customer'] = df['avg_units_sold_30d'] / (df['avg_customers_30d'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to train and test\n",
    "X_train_bal_fe = create_features(X_train_bal)\n",
    "X_test_fe = create_features(X_test)\n",
    "\n",
    "# Define categorical columns for CatBoost\n",
    "categorical_features = ['product_id', 'product_name', 'category', 'brand',\n",
    "                       'store_id', 'store_name', 'store_location',\n",
    "                       'promotion_type', 'day_of_week', 'season']\n",
    "\n",
    "print(\"Done. Ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b4a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_id', 'category', 'brand', 'store_id', 'store_location', 'base_price', 'discount_rate', 'promotion_type', 'day_of_year', 'month', 'day_of_week', 'season', 'is_holiday', 'avg_units_sold_30d', 'avg_customers_30d', 'actual_price', 'discount_amount', 'units_per_customer']\n",
      "product_id             object\n",
      "category               object\n",
      "brand                  object\n",
      "store_id               object\n",
      "store_location         object\n",
      "base_price            float64\n",
      "discount_rate         float64\n",
      "promotion_type         object\n",
      "day_of_year             int64\n",
      "month                   int64\n",
      "day_of_week            object\n",
      "season                 object\n",
      "is_holiday              int64\n",
      "avg_units_sold_30d      int64\n",
      "avg_customers_30d       int64\n",
      "actual_price          float64\n",
      "discount_amount       float64\n",
      "units_per_customer    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bal_fe.columns.tolist())\n",
    "print(X_train_bal_fe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07fcf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Cleaned both datasets.\n"
     ]
    }
   ],
   "source": [
    "# Remove profit_class if it exists\n",
    "if 'profit_class' in X_train_bal_fe.columns:\n",
    "    X_train_bal_fe = X_train_bal_fe.drop(columns=['profit_class'])\n",
    "    \n",
    "if 'profit_class' in X_test_fe.columns:\n",
    "    X_test_fe = X_test_fe.drop(columns=['profit_class'])\n",
    "\n",
    "print(\"Done. Cleaned both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44796713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Cross-Validation...\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 - F1: 0.7483, Accuracy: 0.7455\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 - F1: 0.7105, Accuracy: 0.7062\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 - F1: 0.7378, Accuracy: 0.7354\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 - F1: 0.7432, Accuracy: 0.7404\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 - F1: 0.7377, Accuracy: 0.7396\n",
      "\n",
      "Mean CV F1 Score: 0.7355 (+/- 0.0131)\n",
      "Mean CV Accuracy: 0.7334 (+/- 0.0140)\n",
      "\n",
      "Training final model on full training set...\n",
      "0:\tlearn: 0.5698325\ttotal: 97.6ms\tremaining: 48.7s\n",
      "100:\tlearn: 0.7427289\ttotal: 8.93s\tremaining: 35.3s\n",
      "200:\tlearn: 0.7526858\ttotal: 18.6s\tremaining: 27.6s\n",
      "300:\tlearn: 0.7628331\ttotal: 27.6s\tremaining: 18.2s\n",
      "400:\tlearn: 0.7754366\ttotal: 36.6s\tremaining: 9.03s\n",
      "499:\tlearn: 0.7874684\ttotal: 45.4s\tremaining: 0us\n",
      "\n",
      "==================================================\n",
      "TEST SET RESULTS:\n",
      "==================================================\n",
      "Accuracy: 0.5453\n",
      "F1 Score (Macro): 0.4760\n",
      "Log Loss: 0.8998\n",
      "\n",
      "==================================================\n",
      "CLASSIFICATION REPORT:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.61      0.76      0.68      1537\n",
      "        loss       0.78      0.71      0.75       780\n",
      "         low       0.33      0.03      0.05       705\n",
      "      medium       0.39      0.48      0.43      1478\n",
      "\n",
      "    accuracy                           0.55      4500\n",
      "   macro avg       0.53      0.50      0.48      4500\n",
      "weighted avg       0.52      0.55      0.51      4500\n",
      "\n",
      "==================================================\n",
      "CONFUSION MATRIX:\n",
      "==================================================\n",
      "[[1175    0    2  360]\n",
      " [   0  557   22  201]\n",
      " [  14  123   19  549]\n",
      " [ 729   32   14  703]]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Get categorical column indices\n",
    "cat_features = ['product_id', 'category', 'brand',\n",
    "                'store_id', 'store_location',\n",
    "                'promotion_type', 'day_of_week', 'season']\n",
    "\n",
    "# Find the indices of categorical columns\n",
    "cat_feature_indices = [X_train_bal_fe.columns.get_loc(col) for col in cat_features if col in X_train_bal_fe.columns]\n",
    "\n",
    "# Initialize CatBoost\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='TotalF1',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    cat_features=cat_feature_indices\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_f1_scores = []\n",
    "cv_acc_scores = []\n",
    "\n",
    "print(\"Starting 5-Fold Cross-Validation...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_bal_fe, y_train_bal), 1):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "    \n",
    "    X_tr, X_val = X_train_bal_fe.iloc[train_idx], X_train_bal_fe.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_bal.iloc[train_idx], y_train_bal.iloc[val_idx]\n",
    "    \n",
    "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    cv_f1_scores.append(f1)\n",
    "    cv_acc_scores.append(acc)\n",
    "    \n",
    "    print(f\"Fold {fold} - F1: {f1:.4f}, Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(f\"Mean CV F1 Score: {np.mean(cv_f1_scores):.4f} (+/- {np.std(cv_f1_scores):.4f})\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_acc_scores):.4f} (+/- {np.std(cv_acc_scores):.4f})\")\n",
    "\n",
    "# Train final model on full training data\n",
    "print(\"\\nTraining final model on full training set...\")\n",
    "model.fit(X_train_bal_fe, y_train_bal, verbose=100)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = model.predict(X_test_fe)\n",
    "y_pred_proba = model.predict_proba(X_test_fe)\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_loss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"F1 Score (Macro): {test_f1:.4f}\")\n",
    "print(f\"Log Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(\"=\"*50)\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31cdd38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-27 19:59:00,378] A new study created in memory with name: no-name-3aa3e3b8-455e-4d55-a320-604c54a6b13a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization (50 trials)...\n",
      "This may take 15-30 minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6b5bc8c30142c18d990bf33e50e0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-27 19:59:36,067] Trial 0 finished with value: 0.7258198151753793 and parameters: {'iterations': 726, 'learning_rate': 0.09228716181658375, 'depth': 4, 'l2_leaf_reg': 1.9634180895858169, 'min_data_in_leaf': 25, 'random_strength': 3.185693805681514, 'bagging_temperature': 0.9043203123343585}. Best is trial 0 with value: 0.7258198151753793.\n",
      "[I 2025-12-27 19:59:52,539] Trial 1 finished with value: 0.7227750501984507 and parameters: {'iterations': 625, 'learning_rate': 0.2619653979786518, 'depth': 4, 'l2_leaf_reg': 2.6402266781380566, 'min_data_in_leaf': 47, 'random_strength': 3.335058172422656, 'bagging_temperature': 0.31653060663520516}. Best is trial 0 with value: 0.7258198151753793.\n",
      "[I 2025-12-27 20:00:22,787] Trial 2 finished with value: 0.7285454435894693 and parameters: {'iterations': 327, 'learning_rate': 0.2493906296934598, 'depth': 5, 'l2_leaf_reg': 5.918379329432392, 'min_data_in_leaf': 6, 'random_strength': 7.3530786657702425, 'bagging_temperature': 0.4916301029942548}. Best is trial 2 with value: 0.7285454435894693.\n",
      "[I 2025-12-27 20:00:46,931] Trial 3 finished with value: 0.7281383549563453 and parameters: {'iterations': 462, 'learning_rate': 0.17558789923647714, 'depth': 4, 'l2_leaf_reg': 7.20381947752733, 'min_data_in_leaf': 21, 'random_strength': 6.378448463598426, 'bagging_temperature': 0.8746674791210097}. Best is trial 2 with value: 0.7285454435894693.\n",
      "[I 2025-12-27 20:01:58,686] Trial 4 finished with value: 0.7293034815690015 and parameters: {'iterations': 501, 'learning_rate': 0.03459722048679689, 'depth': 5, 'l2_leaf_reg': 9.222379690744624, 'min_data_in_leaf': 46, 'random_strength': 4.5071622852493665, 'bagging_temperature': 0.5145277517887099}. Best is trial 4 with value: 0.7293034815690015.\n",
      "[I 2025-12-27 20:02:25,298] Trial 5 finished with value: 0.7196593492353655 and parameters: {'iterations': 544, 'learning_rate': 0.19631896779739003, 'depth': 5, 'l2_leaf_reg': 5.390951857464169, 'min_data_in_leaf': 35, 'random_strength': 3.1923214480746056, 'bagging_temperature': 0.6136631646075389}. Best is trial 4 with value: 0.7293034815690015.\n",
      "[I 2025-12-27 20:02:44,855] Trial 6 finished with value: 0.7168083660146377 and parameters: {'iterations': 398, 'learning_rate': 0.21367392862190882, 'depth': 5, 'l2_leaf_reg': 3.4888268304697068, 'min_data_in_leaf': 39, 'random_strength': 0.2973830587291104, 'bagging_temperature': 0.08751128680069997}. Best is trial 4 with value: 0.7293034815690015.\n",
      "[I 2025-12-27 20:03:44,431] Trial 7 finished with value: 0.6739993360608193 and parameters: {'iterations': 767, 'learning_rate': 0.023811913714962857, 'depth': 4, 'l2_leaf_reg': 6.413179572535293, 'min_data_in_leaf': 7, 'random_strength': 4.170680869605054, 'bagging_temperature': 0.6463776318130805}. Best is trial 4 with value: 0.7293034815690015.\n",
      "[I 2025-12-27 20:04:25,905] Trial 8 finished with value: 0.7293576580975331 and parameters: {'iterations': 734, 'learning_rate': 0.1774943664690825, 'depth': 8, 'l2_leaf_reg': 5.991622450087885, 'min_data_in_leaf': 4, 'random_strength': 6.149692473731657, 'bagging_temperature': 0.5417145450821739}. Best is trial 8 with value: 0.7293576580975331.\n",
      "[I 2025-12-27 20:04:54,494] Trial 9 finished with value: 0.720176965753223 and parameters: {'iterations': 418, 'learning_rate': 0.22180451058904338, 'depth': 5, 'l2_leaf_reg': 7.528792473718522, 'min_data_in_leaf': 41, 'random_strength': 3.67603100928335, 'bagging_temperature': 0.8816626907988847}. Best is trial 8 with value: 0.7293576580975331.\n",
      "[I 2025-12-27 20:05:47,317] Trial 10 finished with value: 0.7261936505579855 and parameters: {'iterations': 649, 'learning_rate': 0.12115565592257435, 'depth': 8, 'l2_leaf_reg': 4.295382199144648, 'min_data_in_leaf': 14, 'random_strength': 9.708133471458046, 'bagging_temperature': 0.19861493185756218}. Best is trial 8 with value: 0.7293576580975331.\n",
      "[I 2025-12-27 20:08:25,727] Trial 11 finished with value: 0.733139399102913 and parameters: {'iterations': 541, 'learning_rate': 0.02454930346121948, 'depth': 8, 'l2_leaf_reg': 9.636043128151139, 'min_data_in_leaf': 1, 'random_strength': 6.151445105371493, 'bagging_temperature': 0.43718677245737253}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:09:23,149] Trial 12 finished with value: 0.7258713349800385 and parameters: {'iterations': 619, 'learning_rate': 0.13214315067890667, 'depth': 8, 'l2_leaf_reg': 9.182768836337786, 'min_data_in_leaf': 3, 'random_strength': 6.697481401638717, 'bagging_temperature': 0.4009336106281908}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:10:22,028] Trial 13 finished with value: 0.7268178113359 and parameters: {'iterations': 680, 'learning_rate': 0.07839742995171892, 'depth': 7, 'l2_leaf_reg': 9.733740957857117, 'min_data_in_leaf': 14, 'random_strength': 9.2205500425876, 'bagging_temperature': 0.7059807398562381}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:11:00,633] Trial 14 finished with value: 0.7250488999976716 and parameters: {'iterations': 792, 'learning_rate': 0.15558877139470434, 'depth': 7, 'l2_leaf_reg': 8.049347682494265, 'min_data_in_leaf': 14, 'random_strength': 8.089410742571069, 'bagging_temperature': 0.288905881359816}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:12:16,397] Trial 15 finished with value: 0.7325906917235371 and parameters: {'iterations': 568, 'learning_rate': 0.056116695083469344, 'depth': 7, 'l2_leaf_reg': 1.2107501508011334, 'min_data_in_leaf': 3, 'random_strength': 5.750380423962994, 'bagging_temperature': 0.7431498277030689}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:13:32,839] Trial 16 finished with value: 0.732339989265955 and parameters: {'iterations': 556, 'learning_rate': 0.05765922471398459, 'depth': 7, 'l2_leaf_reg': 1.3227206367270448, 'min_data_in_leaf': 1, 'random_strength': 5.37034172518119, 'bagging_temperature': 0.7397904714610426}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:15:12,606] Trial 17 finished with value: 0.7284357769624544 and parameters: {'iterations': 572, 'learning_rate': 0.017491933023964984, 'depth': 6, 'l2_leaf_reg': 4.569195506961622, 'min_data_in_leaf': 10, 'random_strength': 1.6877947778179134, 'bagging_temperature': 0.7749836762126836}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:16:26,794] Trial 18 finished with value: 0.7321003506351823 and parameters: {'iterations': 486, 'learning_rate': 0.06050983892892276, 'depth': 7, 'l2_leaf_reg': 3.158109947455973, 'min_data_in_leaf': 21, 'random_strength': 7.927964965127408, 'bagging_temperature': 0.03750467895541931}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:16:54,219] Trial 19 finished with value: 0.7182150891954482 and parameters: {'iterations': 316, 'learning_rate': 0.29441755890420873, 'depth': 6, 'l2_leaf_reg': 8.355309469775166, 'min_data_in_leaf': 30, 'random_strength': 5.0890066009836215, 'bagging_temperature': 0.41476608693324435}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:17:52,743] Trial 20 finished with value: 0.730713790181395 and parameters: {'iterations': 591, 'learning_rate': 0.10549728582473034, 'depth': 8, 'l2_leaf_reg': 1.2168141070123726, 'min_data_in_leaf': 10, 'random_strength': 5.4689230444478, 'bagging_temperature': 0.9660344425665114}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:18:58,610] Trial 21 finished with value: 0.7293240392048471 and parameters: {'iterations': 533, 'learning_rate': 0.06301761387911292, 'depth': 7, 'l2_leaf_reg': 1.185382724652831, 'min_data_in_leaf': 3, 'random_strength': 5.520807393699115, 'bagging_temperature': 0.7503589922753642}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:20:06,129] Trial 22 finished with value: 0.7319448903073393 and parameters: {'iterations': 529, 'learning_rate': 0.05009644359553289, 'depth': 6, 'l2_leaf_reg': 2.1919633720725766, 'min_data_in_leaf': 1, 'random_strength': 6.880180989665724, 'bagging_temperature': 0.7915938407800343}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:21:21,242] Trial 23 finished with value: 0.6523124350000469 and parameters: {'iterations': 446, 'learning_rate': 0.010120805731334157, 'depth': 7, 'l2_leaf_reg': 1.036526608246095, 'min_data_in_leaf': 9, 'random_strength': 8.484433336015343, 'bagging_temperature': 0.6192944183905398}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:23:12,609] Trial 24 finished with value: 0.7327262444234415 and parameters: {'iterations': 591, 'learning_rate': 0.04223189462325057, 'depth': 8, 'l2_leaf_reg': 4.318481875871376, 'min_data_in_leaf': 2, 'random_strength': 2.265774622682966, 'bagging_temperature': 0.9958112170579798}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:24:43,464] Trial 25 finished with value: 0.7285468749837013 and parameters: {'iterations': 665, 'learning_rate': 0.041235795667136026, 'depth': 8, 'l2_leaf_reg': 4.6333028118643025, 'min_data_in_leaf': 18, 'random_strength': 2.2813177723118887, 'bagging_temperature': 0.84076553607569}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:25:31,283] Trial 26 finished with value: 0.7173338214526276 and parameters: {'iterations': 607, 'learning_rate': 0.08283507598209376, 'depth': 8, 'l2_leaf_reg': 6.905927211210875, 'min_data_in_leaf': 6, 'random_strength': 0.16438195850270265, 'bagging_temperature': 0.9993662275446}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:26:04,428] Trial 27 finished with value: 0.7239599726858241 and parameters: {'iterations': 691, 'learning_rate': 0.10958485123593581, 'depth': 8, 'l2_leaf_reg': 3.5481699755082143, 'min_data_in_leaf': 12, 'random_strength': 1.5134455893875427, 'bagging_temperature': 0.42311447884043984}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:26:48,408] Trial 28 finished with value: 0.724931737501395 and parameters: {'iterations': 509, 'learning_rate': 0.1415441460102826, 'depth': 7, 'l2_leaf_reg': 4.988970608570342, 'min_data_in_leaf': 1, 'random_strength': 4.587306934427044, 'bagging_temperature': 0.9403882368404785}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:27:52,843] Trial 29 finished with value: 0.7269273733159046 and parameters: {'iterations': 579, 'learning_rate': 0.07444808827046881, 'depth': 8, 'l2_leaf_reg': 2.248120796990297, 'min_data_in_leaf': 23, 'random_strength': 2.065809084629262, 'bagging_temperature': 0.6697258888256064}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:28:48,613] Trial 30 finished with value: 0.7261705754698272 and parameters: {'iterations': 367, 'learning_rate': 0.0959366590045164, 'depth': 7, 'l2_leaf_reg': 2.9600589729873183, 'min_data_in_leaf': 17, 'random_strength': 5.7824469797117795, 'bagging_temperature': 0.5833285195043605}. Best is trial 11 with value: 0.733139399102913.\n",
      "[I 2025-12-27 20:30:36,086] Trial 31 finished with value: 0.7343127918789376 and parameters: {'iterations': 556, 'learning_rate': 0.03969838833399268, 'depth': 7, 'l2_leaf_reg': 1.7396745284529889, 'min_data_in_leaf': 1, 'random_strength': 4.949194963845279, 'bagging_temperature': 0.7989941146403203}. Best is trial 31 with value: 0.7343127918789376.\n",
      "[I 2025-12-27 20:33:17,478] Trial 32 finished with value: 0.7344365873972188 and parameters: {'iterations': 633, 'learning_rate': 0.03325905173738791, 'depth': 8, 'l2_leaf_reg': 3.9411816724565907, 'min_data_in_leaf': 5, 'random_strength': 7.217150318438494, 'bagging_temperature': 0.8427570153724591}. Best is trial 32 with value: 0.7344365873972188.\n",
      "[I 2025-12-27 20:36:08,153] Trial 33 finished with value: 0.7304622189410147 and parameters: {'iterations': 640, 'learning_rate': 0.03135785726807337, 'depth': 8, 'l2_leaf_reg': 4.110841769237219, 'min_data_in_leaf': 8, 'random_strength': 7.381456034887943, 'bagging_temperature': 0.8177886006483046}. Best is trial 32 with value: 0.7344365873972188.\n",
      "[I 2025-12-27 20:38:35,858] Trial 34 finished with value: 0.7319386490137525 and parameters: {'iterations': 599, 'learning_rate': 0.032312590732223305, 'depth': 8, 'l2_leaf_reg': 3.8953522559850287, 'min_data_in_leaf': 5, 'random_strength': 7.273618108570306, 'bagging_temperature': 0.9253505255386955}. Best is trial 32 with value: 0.7344365873972188.\n",
      "[I 2025-12-27 20:40:47,606] Trial 35 finished with value: 0.7278191096857478 and parameters: {'iterations': 711, 'learning_rate': 0.01473018201728788, 'depth': 8, 'l2_leaf_reg': 2.9089901059026086, 'min_data_in_leaf': 28, 'random_strength': 0.9414739810711481, 'bagging_temperature': 0.8657888857758655}. Best is trial 32 with value: 0.7344365873972188.\n",
      "[I 2025-12-27 20:42:14,405] Trial 36 finished with value: 0.7298695577798733 and parameters: {'iterations': 463, 'learning_rate': 0.04142884876456242, 'depth': 8, 'l2_leaf_reg': 5.255891891926896, 'min_data_in_leaf': 6, 'random_strength': 2.860154152626659, 'bagging_temperature': 0.31969516500152284}. Best is trial 32 with value: 0.7344365873972188.\n",
      "[W 2025-12-27 20:42:34,441] Trial 37 failed with parameters: {'iterations': 518, 'learning_rate': 0.07602949305516453, 'depth': 7, 'l2_leaf_reg': 6.0817588053717975, 'min_data_in_leaf': 11, 'random_strength': 3.904461048089924, 'bagging_temperature': 0.9141830061505097} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fatim\\AppData\\Local\\Temp\\ipykernel_21132\\3212271336.py\", line 36, in objective\n",
      "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
      "  File \"c:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2025-12-27 20:42:34,475] Trial 37 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take 15-30 minutes...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBEST HYPERPARAMETERS:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     33\u001b[39m y_tr, y_val = y_train_bal.iloc[train_idx], y_train_bal.iloc[val_idx]\n\u001b[32m     35\u001b[39m model = CatBoostClassifier(**params)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m y_pred = model.predict(X_val)\n\u001b[32m     39\u001b[39m f1 = f1_score(y_val, y_pred, average=\u001b[33m'\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:5245\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5246\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5247\u001b[39m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Get categorical column indices\n",
    "cat_features = ['product_id', 'category', 'brand', 'store_id', 'store_location',\n",
    "                'promotion_type', 'day_of_week', 'season']\n",
    "cat_feature_indices = [X_train_bal_fe.columns.get_loc(col) for col in cat_features if col in X_train_bal_fe.columns]\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 300, 800),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'depth': trial.suggest_int('depth', 4, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'loss_function': 'MultiClass',\n",
    "        'eval_metric': 'TotalF1',\n",
    "        'random_seed': 42,\n",
    "        'verbose': False,\n",
    "        'cat_features': cat_feature_indices\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train_bal_fe, y_train_bal):\n",
    "        X_tr, X_val = X_train_bal_fe.iloc[train_idx], X_train_bal_fe.iloc[val_idx]\n",
    "        y_tr, y_val = y_train_bal.iloc[train_idx], y_train_bal.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro')\n",
    "        cv_scores.append(f1)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "print(\"Starting hyperparameter optimization (50 trials)...\")\n",
    "print(\"This may take 15-30 minutes...\\n\")\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST HYPERPARAMETERS:\")\n",
    "print(\"=\"*50)\n",
    "print(study.best_params)\n",
    "print(f\"\\nBest CV F1 Score: {study.best_value:.4f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training final model with best parameters...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params['loss_function'] = 'MultiClass'\n",
    "best_params['eval_metric'] = 'TotalF1'\n",
    "best_params['random_seed'] = 42\n",
    "best_params['verbose'] = 100\n",
    "best_params['cat_features'] = cat_feature_indices\n",
    "\n",
    "final_model = CatBoostClassifier(**best_params)\n",
    "final_model.fit(X_train_bal_fe, y_train_bal)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, log_loss, classification_report, confusion_matrix\n",
    "\n",
    "y_pred_test = final_model.predict(X_test_fe)\n",
    "y_pred_proba = final_model.predict_proba(X_test_fe)\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_loss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TUNED MODEL - TEST SET RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"F1 Score (Macro): {test_f1:.4f}\")\n",
    "print(f\"Log Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(\"=\"*50)\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d1615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set distribution:\n",
      "profit_class_quantile\n",
      "high      1537\n",
      "medium    1478\n",
      "loss       780\n",
      "low        705\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train set distribution:\n",
      "profit_class_quantile\n",
      "high      7172\n",
      "loss      6901\n",
      "low       6901\n",
      "medium    6901\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set distribution:\")\n",
    "print(y_test.value_counts())\n",
    "print(\"\\nTrain set distribution:\")\n",
    "print(y_train_bal.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f425b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {'high': np.float64(0.7320133853876185), 'loss': np.float64(1.441911562757484), 'low': np.float64(1.5976871576384661), 'medium': np.float64(0.7607593102448921)}\n",
      "0:\tlearn: 1.2945069\ttotal: 205ms\tremaining: 1m 42s\n",
      "100:\tlearn: 0.7788175\ttotal: 8.41s\tremaining: 33.2s\n",
      "200:\tlearn: 0.7446615\ttotal: 17.8s\tremaining: 26.4s\n",
      "300:\tlearn: 0.7147076\ttotal: 26s\tremaining: 17.2s\n",
      "400:\tlearn: 0.6869111\ttotal: 34.9s\tremaining: 8.61s\n",
      "499:\tlearn: 0.6606290\ttotal: 42.1s\tremaining: 0us\n",
      "\n",
      "Test Accuracy: 0.5395555555555556\n",
      "Test F1: 0.5252926000212195\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        high       0.62      0.74      0.67      1537\n",
      "        loss       0.70      0.81      0.75       780\n",
      "         low       0.35      0.62      0.44       705\n",
      "      medium       0.45      0.16      0.23      1478\n",
      "\n",
      "    accuracy                           0.54      4500\n",
      "   macro avg       0.53      0.58      0.53      4500\n",
      "weighted avg       0.53      0.54      0.51      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights from original imbalanced training data\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Create dictionary\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# Train CatBoost with class weights (NO resampling)\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_features = ['product_id', 'category', 'brand', 'store_id', 'store_location',\n",
    "                'promotion_type', 'day_of_week', 'season']\n",
    "cat_feature_indices = [X_train.columns.get_loc(col) for col in cat_features if col in X_train.columns]\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    class_weights=class_weight_dict,\n",
    "    cat_features=cat_feature_indices,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Use ORIGINAL unbalanced training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fe34ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "profit_class_3\n",
      "profit_high    10287\n",
      "profit_low      7072\n",
      "loss            3641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training percentages:\n",
      "profit_class_3\n",
      "profit_high    48.985714\n",
      "profit_low     33.676190\n",
      "loss           17.338095\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "Test set distribution:\n",
      "profit_class_3\n",
      "profit_high    2204\n",
      "profit_low     1515\n",
      "loss            781\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test percentages:\n",
      "profit_class_3\n",
      "profit_high    48.977778\n",
      "profit_low     33.666667\n",
      "loss           17.355556\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTraining percentages:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Test set distribution:\")\n",
    "print(y_test.value_counts())\n",
    "print(\"\\nTest percentages:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b7d2bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3421211\ttotal: 66.8ms\tremaining: 20s\n",
      "100:\tlearn: 0.8039161\ttotal: 4.7s\tremaining: 9.27s\n",
      "200:\tlearn: 0.7923870\ttotal: 9.76s\tremaining: 4.81s\n",
      "299:\tlearn: 0.7827788\ttotal: 14.7s\tremaining: 0us\n",
      "\n",
      "Test Accuracy: 0.5457777777777778\n",
      "Test F1: 0.5362971024877657\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        high       0.63      0.71      0.67      1537\n",
      "        loss       0.71      0.84      0.77       780\n",
      "         low       0.36      0.63      0.45       705\n",
      "      medium       0.45      0.18      0.25      1478\n",
      "\n",
      "    accuracy                           0.55      4500\n",
      "   macro avg       0.54      0.59      0.54      4500\n",
      "weighted avg       0.54      0.55      0.52      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "cat_features = ['product_id', 'category', 'brand', 'store_id', 'store_location',\n",
    "                'promotion_type', 'day_of_week', 'season']\n",
    "cat_feature_indices = [X_train.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=300,  # Reduced from 500\n",
    "    learning_rate=0.05,  # Slower learning\n",
    "    depth=4,  # Shallower trees\n",
    "    l2_leaf_reg=10,  # Strong regularization\n",
    "    min_data_in_leaf=50,  # More data per leaf\n",
    "    class_weights=class_weight_dict,\n",
    "    cat_features=cat_feature_indices,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d1b53b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3429992\ttotal: 62.3ms\tremaining: 18.6s\n",
      "100:\tlearn: 0.8024017\ttotal: 6.84s\tremaining: 13.5s\n",
      "200:\tlearn: 0.7880780\ttotal: 13.7s\tremaining: 6.76s\n",
      "299:\tlearn: 0.7761722\ttotal: 20.5s\tremaining: 0us\n",
      "\n",
      "Test Accuracy: 0.5446666666666666\n",
      "Test F1: 0.5319873984522631\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        high       0.63      0.72      0.67      1537\n",
      "        loss       0.69      0.85      0.76       780\n",
      "         low       0.35      0.59      0.44       705\n",
      "      medium       0.45      0.18      0.25      1478\n",
      "\n",
      "    accuracy                           0.54      4500\n",
      "   macro avg       0.53      0.59      0.53      4500\n",
      "weighted avg       0.54      0.54      0.51      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced feature engineering\n",
    "def create_better_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Price features\n",
    "    df['actual_price'] = df['base_price'] * (1 - df['discount_rate'])\n",
    "    df['discount_amount'] = df['base_price'] * df['discount_rate']\n",
    "    df['price_per_unit_sold'] = df['base_price'] / (df['avg_units_sold_30d'] + 1)\n",
    "    \n",
    "    # Revenue features\n",
    "    df['total_revenue'] = df['actual_price'] * df['avg_units_sold_30d']\n",
    "    df['revenue_per_customer'] = df['total_revenue'] / (df['avg_customers_30d'] + 1)\n",
    "    \n",
    "    # Efficiency features\n",
    "    df['units_per_customer'] = df['avg_units_sold_30d'] / (df['avg_customers_30d'] + 1)\n",
    "    df['customer_to_units_ratio'] = df['avg_customers_30d'] / (df['avg_units_sold_30d'] + 1)\n",
    "    \n",
    "    # Discount features\n",
    "    df['has_discount'] = (df['discount_rate'] > 0).astype(int)\n",
    "    df['high_discount'] = (df['discount_rate'] > 0.3).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to train and test\n",
    "X_train_fe = create_better_features(X_train)\n",
    "X_test_fe = create_better_features(X_test)\n",
    "\n",
    "# Train model\n",
    "cat_feature_indices = [X_train_fe.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    learning_rate=0.05,\n",
    "    depth=5,\n",
    "    l2_leaf_reg=8,\n",
    "    min_data_in_leaf=40,\n",
    "    class_weights=class_weight_dict,\n",
    "    cat_features=cat_feature_indices,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "model.fit(X_train_fe, y_train)\n",
    "y_pred = model.predict(X_test_fe)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "946b12c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profit margin ranges by class:\n",
      "                         min    max      mean       std\n",
      "profit_class_quantile                                  \n",
      "high                   0.312  0.500  0.394898  0.053539\n",
      "loss                  -0.500 -0.001 -0.218517  0.164788\n",
      "low                    0.000  0.158  0.086637  0.045453\n",
      "medium                 0.159  0.311  0.242190  0.040426\n",
      "\n",
      "==================================================\n",
      "Class boundaries:\n",
      "loss: < 0\n",
      "low: 0 to 0.2240\n",
      "medium: 0.2240 to 0.3370\n",
      "high: > 0.3370\n"
     ]
    }
   ],
   "source": [
    "# Check profit margin ranges for each class\n",
    "print(\"Profit margin ranges by class:\")\n",
    "print(df.groupby('profit_class_quantile')['profit_margin'].agg(['min', 'max', 'mean', 'std']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Class boundaries:\")\n",
    "print(f\"loss: < 0\")\n",
    "print(f\"low: 0 to {df[df['profit_margin'] >= 0]['profit_margin'].quantile(0.33):.4f}\")\n",
    "print(f\"medium: {df[df['profit_margin'] >= 0]['profit_margin'].quantile(0.33):.4f} to {df[df['profit_margin'] >= 0]['profit_margin'].quantile(0.66):.4f}\")\n",
    "print(f\"high: > {df[df['profit_margin'] >= 0]['profit_margin'].quantile(0.66):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8d6372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many samples in original df: 30000\n",
      "\n",
      "Actual value counts:\n",
      "profit_class_quantile\n",
      "high      10245\n",
      "loss       5202\n",
      "low        4695\n",
      "medium     9858\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quantiles calculated from non-negative values:\n",
      "Q1 (33rd percentile): 0.2240\n",
      "Q2 (66th percentile): 0.3370\n",
      "\n",
      "Samples near low/medium boundary:\n",
      "     profit_margin profit_class_quantile\n",
      "14           0.165                medium\n",
      "50           0.166                medium\n",
      "97           0.152                   low\n",
      "175          0.161                medium\n",
      "179          0.165                medium\n",
      "347          0.154                   low\n",
      "361          0.151                   low\n",
      "404          0.153                   low\n",
      "447          0.158                   low\n",
      "563          0.167                medium\n"
     ]
    }
   ],
   "source": [
    "# Check how you actually created the classes\n",
    "print(\"How many samples in original df:\", len(df))\n",
    "print(\"\\nActual value counts:\")\n",
    "print(df['profit_class_quantile'].value_counts().sort_index())\n",
    "\n",
    "# Verify the thresholds you used\n",
    "non_negative = df[df['profit_margin'] >= 0]['profit_margin']\n",
    "q1_actual = non_negative.quantile(0.33)\n",
    "q2_actual = non_negative.quantile(0.66)\n",
    "\n",
    "print(f\"\\nQuantiles calculated from non-negative values:\")\n",
    "print(f\"Q1 (33rd percentile): {q1_actual:.4f}\")\n",
    "print(f\"Q2 (66th percentile): {q2_actual:.4f}\")\n",
    "\n",
    "# Check a few examples around boundaries\n",
    "print(\"\\nSamples near low/medium boundary:\")\n",
    "print(df[(df['profit_margin'] > 0.15) & (df['profit_margin'] < 0.17)][['profit_margin', 'profit_class_quantile']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f227c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct thresholds: q1=0.2240, q2=0.3370\n",
      "\n",
      "New class distribution:\n",
      "profit_class_correct\n",
      "high      8491\n",
      "loss      5202\n",
      "low       8108\n",
      "medium    8199\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: (21000, 16) Test: (4500, 16)\n"
     ]
    }
   ],
   "source": [
    "# Recalculate quantiles correctly\n",
    "non_negative = df[df['profit_margin'] >= 0]['profit_margin']\n",
    "q1 = non_negative.quantile(0.33)\n",
    "q2 = non_negative.quantile(0.66)\n",
    "\n",
    "print(f\"Correct thresholds: q1={q1:.4f}, q2={q2:.4f}\")\n",
    "\n",
    "# Recreate classes\n",
    "def profit_bucket_correct(x):\n",
    "    if x < 0:\n",
    "        return \"loss\"\n",
    "    elif x < q1:\n",
    "        return \"low\"\n",
    "    elif x < q2:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "# Apply correct bucketing\n",
    "df['profit_class_correct'] = df['profit_margin'].apply(profit_bucket_correct)\n",
    "\n",
    "print(\"\\nNew class distribution:\")\n",
    "print(df['profit_class_correct'].value_counts().sort_index())\n",
    "\n",
    "# Now redo everything with correct classes\n",
    "X = df.drop(columns=[\"profit_margin\", \"profit_class\", \"profit_class_quantile\", \n",
    "                     \"profit_class_correct\", \"product_name\", \"store_name\"])\n",
    "y = df[\"profit_class_correct\"]\n",
    "\n",
    "# Split again\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(\"\\nTrain:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef73e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3173124\ttotal: 71.9ms\tremaining: 28.7s\n",
      "100:\tlearn: 0.8343247\ttotal: 7.85s\tremaining: 23.3s\n",
      "200:\tlearn: 0.8101235\ttotal: 16.1s\tremaining: 16s\n",
      "300:\tlearn: 0.7917254\ttotal: 24.3s\tremaining: 7.99s\n",
      "399:\tlearn: 0.7738830\ttotal: 32.3s\tremaining: 0us\n",
      "\n",
      "==================================================\n",
      "TEST RESULTS WITH CORRECT CLASSES:\n",
      "==================================================\n",
      "Accuracy: 0.5547\n",
      "F1 Score: 0.5150\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        high       0.52      0.82      0.64      1274\n",
      "        loss       0.71      0.87      0.78       780\n",
      "         low       0.51      0.58      0.54      1216\n",
      "      medium       0.41      0.05      0.10      1230\n",
      "\n",
      "    accuracy                           0.55      4500\n",
      "   macro avg       0.54      0.58      0.52      4500\n",
      "weighted avg       0.52      0.55      0.49      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Define categorical features\n",
    "cat_features = ['product_id', 'category', 'brand', 'store_id', 'store_location',\n",
    "                'promotion_type', 'day_of_week', 'season']\n",
    "cat_feature_indices = [X_train.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "# Train model\n",
    "model = CatBoostClassifier(\n",
    "    iterations=400,\n",
    "    learning_rate=0.08,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=5,\n",
    "    class_weights=class_weight_dict,\n",
    "    cat_features=cat_feature_indices,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST RESULTS WITH CORRECT CLASSES:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbd93a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New 3-class distribution:\n",
      "profit_class_3\n",
      "loss            5202\n",
      "profit_high    14695\n",
      "profit_low     10103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: (21000, 15) Test: (4500, 15)\n",
      "\n",
      "Train distribution:\n",
      "profit_class_3\n",
      "profit_high    10287\n",
      "profit_low      7072\n",
      "loss            3641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test distribution:\n",
      "profit_class_3\n",
      "profit_high    2204\n",
      "profit_low     1515\n",
      "loss            781\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create 3 classes\n",
    "def profit_bucket_3class(x):\n",
    "    if x < 0:\n",
    "        return \"loss\"\n",
    "    elif x < 0.25:  # Combine low and medium\n",
    "        return \"profit_low\"\n",
    "    else:\n",
    "        return \"profit_high\"\n",
    "\n",
    "# Apply new bucketing\n",
    "df['profit_class_3'] = df['profit_margin'].apply(profit_bucket_3class)\n",
    "\n",
    "print(\"New 3-class distribution:\")\n",
    "print(df['profit_class_3'].value_counts().sort_index())\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"profit_margin\", \"profit_class\", \"profit_class_quantile\", \n",
    "                     \"profit_class_correct\", \"profit_class_3\", \"product_name\", \"store_name\"])\n",
    "y = df[\"profit_class_3\"]\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(\"\\nTrain:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "print(\"\\nTrain distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9da422b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {'loss': np.float64(1.9225487503433123), 'profit_high': np.float64(0.6804704967434626), 'profit_low': np.float64(0.9898190045248869)}\n",
      "0:\tlearn: 1.0212572\ttotal: 46.3ms\tremaining: 23.1s\n",
      "100:\tlearn: 0.5596422\ttotal: 6.39s\tremaining: 25.2s\n",
      "200:\tlearn: 0.5362747\ttotal: 13.1s\tremaining: 19.4s\n",
      "300:\tlearn: 0.5158160\ttotal: 19.6s\tremaining: 13s\n",
      "400:\tlearn: 0.4979527\ttotal: 26.4s\tremaining: 6.51s\n",
      "499:\tlearn: 0.4796469\ttotal: 33.1s\tremaining: 0us\n",
      "\n",
      "==================================================\n",
      "3-CLASS MODEL RESULTS:\n",
      "==================================================\n",
      "Accuracy: 0.7156\n",
      "F1 Score (Macro): 0.7095\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        loss       0.70      0.86      0.77       781\n",
      " profit_high       0.80      0.79      0.80      2204\n",
      "  profit_low       0.59      0.54      0.56      1515\n",
      "\n",
      "    accuracy                           0.72      4500\n",
      "   macro avg       0.70      0.73      0.71      4500\n",
      "weighted avg       0.71      0.72      0.71      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# Define categorical features\n",
    "cat_features = ['product_id', 'category', 'brand', 'store_id', 'store_location',\n",
    "                'promotion_type', 'day_of_week', 'season']\n",
    "cat_feature_indices = [X_train.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "# Train model\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    class_weights=class_weight_dict,\n",
    "    cat_features=cat_feature_indices,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3-CLASS MODEL RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8ae6ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-Fold Cross-Validation...\n",
      "Fold 1: F1 = 0.6940\n",
      "Fold 2: F1 = 0.6915\n",
      "Fold 3: F1 = 0.6944\n",
      "Fold 4: F1 = 0.6820\n",
      "Fold 5: F1 = 0.6912\n",
      "\n",
      "Mean CV F1: 0.6906 (+/- 0.0045)\n",
      "Test F1: 0.7095\n",
      "\n",
      "Good! CV and test scores are close - no major overfitting.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Cross-validation first\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Running 5-Fold Cross-Validation...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    model_cv = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        class_weights=class_weight_dict,\n",
    "        cat_features=cat_feature_indices,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model_cv.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "    y_pred_val = model_cv.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "    cv_scores.append(f1)\n",
    "    print(f\"Fold {fold}: F1 = {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV F1: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(f\"Test F1: 0.7095\")\n",
    "print(\"\\nGood! CV and test scores are close - no major overfitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a080902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-27 21:02:06,311] A new study created in memory with name: no-name-d11bb64e-a5db-481d-86ac-aa947cd26af3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization (30 trials)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462a2ec317e346e8a898bd8514ec09a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-27 21:03:14,391] Trial 0 finished with value: 0.6907800660368619 and parameters: {'iterations': 626, 'learning_rate': 0.09453529012808881, 'depth': 5, 'l2_leaf_reg': 6.4194186812966665, 'min_data_in_leaf': 77}. Best is trial 0 with value: 0.6907800660368619.\n",
      "[I 2025-12-27 21:04:51,673] Trial 1 finished with value: 0.6944373150428857 and parameters: {'iterations': 576, 'learning_rate': 0.07984126706847643, 'depth': 8, 'l2_leaf_reg': 1.6600071994492231, 'min_data_in_leaf': 87}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:05:42,780] Trial 2 finished with value: 0.6899367389950243 and parameters: {'iterations': 614, 'learning_rate': 0.11162756608980867, 'depth': 5, 'l2_leaf_reg': 4.373883459158026, 'min_data_in_leaf': 63}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:06:14,306] Trial 3 finished with value: 0.6903099994332732 and parameters: {'iterations': 503, 'learning_rate': 0.08780675239598174, 'depth': 4, 'l2_leaf_reg': 7.00943726049655, 'min_data_in_leaf': 46}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:07:16,452] Trial 4 finished with value: 0.6888958603585652 and parameters: {'iterations': 520, 'learning_rate': 0.07847151934556384, 'depth': 6, 'l2_leaf_reg': 2.3017988152053936, 'min_data_in_leaf': 71}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:08:16,630] Trial 5 finished with value: 0.6922370922976967 and parameters: {'iterations': 368, 'learning_rate': 0.12786239466529156, 'depth': 8, 'l2_leaf_reg': 6.615853613548879, 'min_data_in_leaf': 90}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:09:24,478] Trial 6 finished with value: 0.6931567739499044 and parameters: {'iterations': 415, 'learning_rate': 0.10158868039320687, 'depth': 7, 'l2_leaf_reg': 1.3627115459773353, 'min_data_in_leaf': 55}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:10:30,472] Trial 7 finished with value: 0.6886149494772998 and parameters: {'iterations': 415, 'learning_rate': 0.1594015455463336, 'depth': 7, 'l2_leaf_reg': 1.4396782957816263, 'min_data_in_leaf': 100}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:11:39,536] Trial 8 finished with value: 0.6924241825418574 and parameters: {'iterations': 463, 'learning_rate': 0.05439975831482281, 'depth': 7, 'l2_leaf_reg': 9.350398213398233, 'min_data_in_leaf': 44}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:12:44,507] Trial 9 finished with value: 0.6885543349541354 and parameters: {'iterations': 435, 'learning_rate': 0.1424695465112903, 'depth': 7, 'l2_leaf_reg': 7.049119023365366, 'min_data_in_leaf': 12}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:14:17,340] Trial 10 finished with value: 0.6839577601815771 and parameters: {'iterations': 562, 'learning_rate': 0.19821612654759427, 'depth': 8, 'l2_leaf_reg': 4.258016205036547, 'min_data_in_leaf': 26}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:15:07,069] Trial 11 finished with value: 0.6924029757168194 and parameters: {'iterations': 305, 'learning_rate': 0.050602121232043315, 'depth': 8, 'l2_leaf_reg': 2.755390765323912, 'min_data_in_leaf': 48}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:16:49,940] Trial 12 finished with value: 0.6891314614104415 and parameters: {'iterations': 666, 'learning_rate': 0.10896603621414568, 'depth': 7, 'l2_leaf_reg': 1.1093695355219912, 'min_data_in_leaf': 81}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:17:49,757] Trial 13 finished with value: 0.6915928596268287 and parameters: {'iterations': 574, 'learning_rate': 0.07270141190886013, 'depth': 6, 'l2_leaf_reg': 3.255093997679847, 'min_data_in_leaf': 60}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:18:51,146] Trial 14 finished with value: 0.6921455780006406 and parameters: {'iterations': 380, 'learning_rate': 0.10614675607327285, 'depth': 8, 'l2_leaf_reg': 4.174020577372406, 'min_data_in_leaf': 25}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:20:11,361] Trial 15 finished with value: 0.6914463965838972 and parameters: {'iterations': 680, 'learning_rate': 0.06926906485532046, 'depth': 6, 'l2_leaf_reg': 1.012661327974654, 'min_data_in_leaf': 100}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:21:31,938] Trial 16 finished with value: 0.68685305735822 and parameters: {'iterations': 535, 'learning_rate': 0.13134035265380536, 'depth': 7, 'l2_leaf_reg': 2.471575536748065, 'min_data_in_leaf': 36}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:22:54,344] Trial 17 finished with value: 0.690241328425281 and parameters: {'iterations': 476, 'learning_rate': 0.16092858979679983, 'depth': 8, 'l2_leaf_reg': 9.101421141255384, 'min_data_in_leaf': 66}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:23:44,870] Trial 18 finished with value: 0.6927922230766445 and parameters: {'iterations': 345, 'learning_rate': 0.09621170530214591, 'depth': 7, 'l2_leaf_reg': 5.54468730885838, 'min_data_in_leaf': 87}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:24:31,162] Trial 19 finished with value: 0.6934390528245492 and parameters: {'iterations': 594, 'learning_rate': 0.0673682658822709, 'depth': 5, 'l2_leaf_reg': 3.393776690872693, 'min_data_in_leaf': 54}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:25:06,943] Trial 20 finished with value: 0.691057713480605 and parameters: {'iterations': 613, 'learning_rate': 0.06347595302071643, 'depth': 4, 'l2_leaf_reg': 3.5979836885345304, 'min_data_in_leaf': 71}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:25:51,512] Trial 21 finished with value: 0.6915313664165854 and parameters: {'iterations': 569, 'learning_rate': 0.0754046951657236, 'depth': 5, 'l2_leaf_reg': 2.003534146494757, 'min_data_in_leaf': 59}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:26:44,579] Trial 22 finished with value: 0.6903857155699681 and parameters: {'iterations': 654, 'learning_rate': 0.08748809106292398, 'depth': 5, 'l2_leaf_reg': 1.8737507141631358, 'min_data_in_leaf': 53}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:27:47,894] Trial 23 finished with value: 0.6912591421533278 and parameters: {'iterations': 595, 'learning_rate': 0.061115383014816956, 'depth': 6, 'l2_leaf_reg': 3.303177536250113, 'min_data_in_leaf': 38}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:28:20,402] Trial 24 finished with value: 0.6919492517956307 and parameters: {'iterations': 542, 'learning_rate': 0.08466087076605687, 'depth': 4, 'l2_leaf_reg': 5.179805565295668, 'min_data_in_leaf': 54}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:29:40,212] Trial 25 finished with value: 0.686910764470276 and parameters: {'iterations': 474, 'learning_rate': 0.11868206389890854, 'depth': 8, 'l2_leaf_reg': 1.7185311476761154, 'min_data_in_leaf': 35}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:30:14,573] Trial 26 finished with value: 0.6891164128472006 and parameters: {'iterations': 435, 'learning_rate': 0.10235989494936512, 'depth': 5, 'l2_leaf_reg': 2.846348730202224, 'min_data_in_leaf': 70}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:31:22,254] Trial 27 finished with value: 0.6906667830453791 and parameters: {'iterations': 644, 'learning_rate': 0.07891748670579507, 'depth': 6, 'l2_leaf_reg': 3.674895133108428, 'min_data_in_leaf': 54}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:33:03,785] Trial 28 finished with value: 0.6917585669962126 and parameters: {'iterations': 694, 'learning_rate': 0.06268648340054413, 'depth': 7, 'l2_leaf_reg': 5.017873131392094, 'min_data_in_leaf': 81}. Best is trial 1 with value: 0.6944373150428857.\n",
      "[I 2025-12-27 21:34:06,547] Trial 29 finished with value: 0.6905038996267437 and parameters: {'iterations': 593, 'learning_rate': 0.09535478038354431, 'depth': 6, 'l2_leaf_reg': 1.7324917483177067, 'min_data_in_leaf': 26}. Best is trial 1 with value: 0.6944373150428857.\n",
      "\n",
      "==================================================\n",
      "BEST PARAMETERS:\n",
      "{'iterations': 576, 'learning_rate': 0.07984126706847643, 'depth': 8, 'l2_leaf_reg': 1.6600071994492231, 'min_data_in_leaf': 87}\n",
      "Best CV F1: 0.6944\n",
      "0:\tlearn: 1.0361173\ttotal: 81.5ms\tremaining: 46.9s\n",
      "100:\tlearn: 0.5467334\ttotal: 8.26s\tremaining: 38.8s\n",
      "200:\tlearn: 0.4995275\ttotal: 17.6s\tremaining: 32.8s\n",
      "300:\tlearn: 0.4569382\ttotal: 27.7s\tremaining: 25.3s\n",
      "400:\tlearn: 0.4203500\ttotal: 37.9s\tremaining: 16.6s\n",
      "500:\tlearn: 0.3857582\ttotal: 47.2s\tremaining: 7.06s\n",
      "575:\tlearn: 0.3634849\ttotal: 54.1s\tremaining: 0us\n",
      "\n",
      "==================================================\n",
      "FINAL TUNED MODEL:\n",
      "==================================================\n",
      "Accuracy: 0.7129\n",
      "F1 Score: 0.7079\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        loss       0.72      0.83      0.77       781\n",
      " profit_high       0.79      0.79      0.79      2204\n",
      "  profit_low       0.58      0.54      0.56      1515\n",
      "\n",
      "    accuracy                           0.71      4500\n",
      "   macro avg       0.70      0.72      0.71      4500\n",
      "weighted avg       0.71      0.71      0.71      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 300, 700),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),\n",
    "        'depth': trial.suggest_int('depth', 4, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'class_weights': class_weight_dict,\n",
    "        'cat_features': cat_feature_indices,\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "    return score.mean()\n",
    "\n",
    "print(\"Starting hyperparameter optimization (30 trials)...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST PARAMETERS:\")\n",
    "print(study.best_params)\n",
    "print(f\"Best CV F1: {study.best_value:.4f}\")\n",
    "\n",
    "# Train final model with best params\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'class_weights': class_weight_dict,\n",
    "    'cat_features': cat_feature_indices,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 100\n",
    "})\n",
    "\n",
    "final_model = CatBoostClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TUNED MODEL:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_final, average='macro'):.4f}\")\n",
    "print(\"\\n\", classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2408cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA PREPARATION & LOGGING\n",
      "============================================================\n",
      " Dataset info logged\n",
      "============================================================\n",
      "STEP 2: CLASS WEIGHTS & PREPROCESSING\n",
      "============================================================\n",
      " Class weights computed and logged\n",
      "============================================================\n",
      "STEP 3: MODEL TRAINING\n",
      "============================================================\n",
      "0:\tlearn: 1.0361173\ttest: 1.0356783\tbest: 1.0356783 (0)\ttotal: 70.6ms\tremaining: 40.6s\n",
      "100:\tlearn: 0.5467334\ttest: 0.5699004\tbest: 0.5699004 (100)\ttotal: 7.56s\tremaining: 35.5s\n",
      "200:\tlearn: 0.4995275\ttest: 0.5669162\tbest: 0.5667703 (190)\ttotal: 15.7s\tremaining: 29.2s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.5667702643\n",
      "bestIteration = 190\n",
      "\n",
      "Shrink model to first 191 iterations.\n",
      " Model trained\n",
      "============================================================\n",
      "STEP 4: MODEL EVALUATION & METRICS\n",
      "============================================================\n",
      "Train Accuracy: 0.7109, F1: 0.7068\n",
      "Val Accuracy: 0.7076, F1: 0.7043\n",
      "Test Accuracy: 0.7109, F1: 0.7047\n",
      "============================================================\n",
      "STEP 5: CONFUSION MATRIX & VISUALIZATIONS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/31 20:58:03 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Visualizations created and logged\n",
      "============================================================\n",
      "STEP 6: MODEL SAVING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/31 20:58:24 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/31 20:58:24 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/31 20:58:24 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/31 20:58:24 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "Successfully registered model 'profit_classifier_catboost'.\n",
      "Created version '1' of model 'profit_classifier_catboost'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved\n",
      "============================================================\n",
      "STEP 7: ADDITIONAL ARTIFACTS FOR DOCKER\n",
      "============================================================\n",
      " Docker artifacts saved\n",
      "============================================================\n",
      "MLflow LOGGING COMPLETE!\n",
      "============================================================\n",
      "Run ID: 85fc8c6fe9254731924a87b76c100212\n",
      "Experiment: profit_classification_experiment\n",
      "Model registered as: profit_classifier_catboost\n",
      "\n",
      "To view results:\n",
      "  mlflow ui\n",
      "  Then open: http://localhost:5000\n",
      "============================================================\n",
      "\n",
      " ALL DONE! Everything logged to MLflow.\n",
      "\n",
      "Next steps:\n",
      "1. Run 'mlflow ui' to view results\n",
      "2. Use the run_id to load the model later\n",
      "3. Check mlruns/ folder for all artifacts\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"Profit_Prediction_Experiment\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"catboost_3class_final\"):\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: DATA PREPARATION & LOGGING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create 3 classes\n",
    "    def profit_bucket_3class(x):\n",
    "        if x < 0:\n",
    "            return \"loss\"\n",
    "        elif x < 0.25:\n",
    "            return \"profit_low\"\n",
    "        else:\n",
    "            return \"profit_high\"\n",
    "    \n",
    "    df['profit_class_3'] = df['profit_margin'].apply(profit_bucket_3class)\n",
    "    \n",
    "    # Log raw dataset info\n",
    "    mlflow.log_param(\"total_samples\", len(df))\n",
    "    mlflow.log_param(\"n_features_raw\", df.shape[1])\n",
    "    mlflow.log_param(\"class_distribution\", df['profit_class_3'].value_counts().to_dict())\n",
    "    \n",
    "    # Log class boundaries\n",
    "    mlflow.log_param(\"loss_boundary\", \"< 0\")\n",
    "    mlflow.log_param(\"profit_low_boundary\", \"0 to 0.25\")\n",
    "    mlflow.log_param(\"profit_high_boundary\", \"> 0.25\")\n",
    "    \n",
    "    # Save raw dataset sample\n",
    "    df.head(100).to_csv(\"data_sample.csv\", index=False)\n",
    "    mlflow.log_artifact(\"data_sample.csv\", \"data\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X = df.drop(columns=[\"profit_margin\", \"profit_class\", \"profit_class_quantile\", \n",
    "                         \"profit_class_correct\", \"profit_class_3\", \"product_name\", \"store_name\"])\n",
    "    y = df[\"profit_class_3\"]\n",
    "    \n",
    "    # Log feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    mlflow.log_param(\"n_features_final\", len(feature_names))\n",
    "    with open(\"feature_names.json\", \"w\") as f:\n",
    "        json.dump(feature_names, f, indent=2)\n",
    "    mlflow.log_artifact(\"feature_names.json\", \"features\")\n",
    "    \n",
    "    # Log categorical features\n",
    "    cat_features = ['product_id', 'category', 'brand', 'store_id', 'store_location',\n",
    "                    'promotion_type', 'day_of_week', 'season']\n",
    "    with open(\"categorical_features.json\", \"w\") as f:\n",
    "        json.dump(cat_features, f, indent=2)\n",
    "    mlflow.log_artifact(\"categorical_features.json\", \"features\")\n",
    "    \n",
    "    print(\" Dataset info logged\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Log split info\n",
    "    mlflow.log_param(\"train_size\", len(X_train))\n",
    "    mlflow.log_param(\"val_size\", len(X_val))\n",
    "    mlflow.log_param(\"test_size\", len(X_test))\n",
    "    mlflow.log_param(\"test_split_ratio\", 0.15)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"stratify\", True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 2: CLASS WEIGHTS & PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "    \n",
    "    # Log class weights\n",
    "    for cls, weight in class_weight_dict.items():\n",
    "        mlflow.log_param(f\"class_weight_{cls}\", float(weight))\n",
    "    \n",
    "    # Get categorical indices\n",
    "    cat_feature_indices = [X_train.columns.get_loc(col) for col in cat_features]\n",
    "    mlflow.log_param(\"categorical_indices\", cat_feature_indices)\n",
    "    \n",
    "    print(\" Class weights computed and logged\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 3: MODEL TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model hyperparameters (use your best params from Optuna)\n",
    "    params = {\n",
    "        'iterations': 576,\n",
    "        'learning_rate': 0.07984126706847643,\n",
    "        'depth': 8,\n",
    "        'l2_leaf_reg': 1.6600071994492231,\n",
    "        'min_data_in_leaf': 87,\n",
    "        'class_weights': class_weight_dict,\n",
    "        'cat_features': cat_feature_indices,\n",
    "        'random_seed': 42,\n",
    "        'verbose': 100\n",
    "    }\n",
    "    \n",
    "    # Log all hyperparameters\n",
    "    for key, value in params.items():\n",
    "        if key not in ['class_weights', 'cat_features']:\n",
    "            mlflow.log_param(key, value)\n",
    "    \n",
    "    # Train model\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    print(\" Model trained\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 4: MODEL EVALUATION & METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics for all sets\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "    \n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_accuracy\", train_acc)\n",
    "    mlflow.log_metric(\"train_f1_macro\", train_f1)\n",
    "    mlflow.log_metric(\"val_accuracy\", val_acc)\n",
    "    mlflow.log_metric(\"val_f1_macro\", val_f1)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1_macro\", test_f1)\n",
    "    \n",
    "    # Per-class metrics for test set\n",
    "    report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    for class_name in ['loss', 'profit_low', 'profit_high']:\n",
    "        if class_name in report:\n",
    "            mlflow.log_metric(f\"test_{class_name}_precision\", report[class_name]['precision'])\n",
    "            mlflow.log_metric(f\"test_{class_name}_recall\", report[class_name]['recall'])\n",
    "            mlflow.log_metric(f\"test_{class_name}_f1\", report[class_name]['f1-score'])\n",
    "    \n",
    "    print(f\"Train Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "    \n",
    "    # Save classification report\n",
    "    report_text = classification_report(y_test, y_test_pred)\n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report_text)\n",
    "    mlflow.log_artifact(\"classification_report.txt\", \"evaluation\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 5: CONFUSION MATRIX & VISUALIZATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['loss', 'profit_high', 'profit_low'],\n",
    "                yticklabels=['loss', 'profit_high', 'profit_low'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix.png\", dpi=150)\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\", \"visualizations\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = model.get_feature_importance()\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save feature importance\n",
    "    importance_df.to_csv(\"feature_importance.csv\", index=False)\n",
    "    mlflow.log_artifact(\"feature_importance.csv\", \"features\")\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\", dpi=150)\n",
    "    mlflow.log_artifact(\"feature_importance.png\", \"visualizations\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\" Visualizations created and logged\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 6: MODEL SAVING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Log model with MLflow\n",
    "    mlflow.catboost.log_model(\n",
    "        model,\n",
    "        \"model\",\n",
    "        registered_model_name=\"profit_classifier_catboost\"\n",
    "    )\n",
    "    \n",
    "    # Also save as native CatBoost format\n",
    "    model.save_model(\"catboost_model.cbm\")\n",
    "    mlflow.log_artifact(\"catboost_model.cbm\", \"model_native\")\n",
    "    \n",
    "    print(\" Model saved\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 7: ADDITIONAL ARTIFACTS FOR DOCKER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save preprocessing info\n",
    "    preprocessing_info = {\n",
    "        \"feature_names\": feature_names,\n",
    "        \"categorical_features\": cat_features,\n",
    "        \"categorical_indices\": cat_feature_indices,\n",
    "        \"class_names\": ['loss', 'profit_high', 'profit_low'],\n",
    "        \"class_weights\": {k: float(v) for k, v in class_weight_dict.items()},\n",
    "        \"profit_boundaries\": {\n",
    "            \"loss\": \"< 0\",\n",
    "            \"profit_low\": \"0 to 0.25\",\n",
    "            \"profit_high\": \"> 0.25\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"preprocessing_config.json\", \"w\") as f:\n",
    "        json.dump(preprocessing_info, f, indent=2)\n",
    "    mlflow.log_artifact(\"preprocessing_config.json\", \"config\")\n",
    "    \n",
    "    # Save model signature for inference\n",
    "    model_signature = {\n",
    "        \"input_features\": feature_names,\n",
    "        \"output_classes\": ['loss', 'profit_high', 'profit_low'],\n",
    "        \"model_type\": \"CatBoostClassifier\",\n",
    "        \"model_version\": \"1.0\"\n",
    "    }\n",
    "    \n",
    "    with open(\"model_signature.json\", \"w\") as f:\n",
    "        json.dump(model_signature, f, indent=2)\n",
    "    mlflow.log_artifact(\"model_signature.json\", \"config\")\n",
    "    \n",
    "    # Save requirements.txt for Docker\n",
    "    requirements = \"\"\"catboost==1.2\n",
    "pandas==2.0.0\n",
    "numpy==1.24.0\n",
    "scikit-learn==1.3.0\n",
    "mlflow==2.9.0\n",
    "\"\"\"\n",
    "    with open(\"requirements.txt\", \"w\") as f:\n",
    "        f.write(requirements)\n",
    "    mlflow.log_artifact(\"requirements.txt\", \"docker\")\n",
    "    \n",
    "    print(\" Docker artifacts saved\")\n",
    "    \n",
    "    # Get run ID for reference\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"MLflow LOGGING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Experiment: profit_classification_experiment\")\n",
    "    print(f\"Model registered as: profit_classifier_catboost\")\n",
    "    print(\"\\nTo view results:\")\n",
    "    print(\"  mlflow ui\")\n",
    "    print(\"  Then open: http://localhost:5000\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"\\n ALL DONE! Everything logged to MLflow.\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run 'mlflow ui' to view results\")\n",
    "print(\"2. Use the run_id to load the model later\")\n",
    "print(\"3. Check mlruns/ folder for all artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b37ba79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fatim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\pyfunc\\utils\\data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0361173\ttest: 1.0356783\tbest: 1.0356783 (0)\ttotal: 175ms\tremaining: 1m 40s\n",
      "100:\tlearn: 0.5467334\ttest: 0.5699004\tbest: 0.5699004 (100)\ttotal: 11.6s\tremaining: 54.4s\n",
      "200:\tlearn: 0.4995275\ttest: 0.5669162\tbest: 0.5667703 (190)\ttotal: 21.6s\tremaining: 40.4s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.5667702637\n",
      "bestIteration = 190\n",
      "\n",
      "Shrink model to first 191 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/01 01:39:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PIPELINE MODEL SAVED & REGISTERED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'profit_classifier_pipeline' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'profit_classifier_pipeline'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 1 PROFIT BUCKET FUNCTION\n",
    "# ===============================\n",
    "def profit_bucket_3class(x):\n",
    "    if x < 0:\n",
    "        return \"loss\"\n",
    "    elif x < 0.25:\n",
    "        return \"profit_low\"\n",
    "    else:\n",
    "        return \"profit_high\"\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2 FEATURE SELECTOR TRANSFORMER\n",
    "# ===============================\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.feature_names]\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3 MLflow PIPELINE MODEL\n",
    "# ===============================\n",
    "class ProfitPipelineModel(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def __init__(self, model, feature_names):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        X = model_input[self.feature_names]\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4 START EXPERIMENT\n",
    "# ===============================\n",
    "mlflow.set_experiment(\"Profit_margin_Prediction_Experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"catboost_pipeline_final\"):\n",
    "\n",
    "    # ===============================\n",
    "    # DATA PREPARATION\n",
    "    # ===============================\n",
    "    df[\"profit_class_3\"] = df[\"profit_margin\"].apply(profit_bucket_3class)\n",
    "\n",
    "    X = df.drop(columns=[\n",
    "        \"profit_margin\",\n",
    "        \"profit_class\",\n",
    "        \"profit_class_quantile\",\n",
    "        \"profit_class_3\",\n",
    "        \"profit_class_correct\",  \n",
    "        \"product_name\",\n",
    "        \"store_name\"\n",
    "    ])\n",
    "\n",
    "    y = df[\"profit_class_3\"]\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    # ===============================\n",
    "    # LOG DATASET METADATA\n",
    "    # ===============================\n",
    "    mlflow.log_param(\"total_samples\", len(df))\n",
    "    mlflow.log_param(\"n_features\", len(feature_names))\n",
    "    mlflow.log_param(\"class_distribution\", y.value_counts().to_dict())\n",
    "\n",
    "    # ===============================\n",
    "    # CATEGORICAL FEATURES\n",
    "    # ===============================\n",
    "    cat_features = [\n",
    "        \"product_id\",\n",
    "        \"category\",\n",
    "        \"brand\",\n",
    "        \"store_id\",\n",
    "        \"store_location\",\n",
    "        \"promotion_type\",\n",
    "        \"day_of_week\",\n",
    "        \"season\"\n",
    "    ]\n",
    "\n",
    "    cat_feature_indices = [feature_names.index(col) for col in cat_features]\n",
    "\n",
    "    # ===============================\n",
    "    # TRAIN / VAL / TEST SPLIT\n",
    "    # ===============================\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # CLASS WEIGHTS\n",
    "    # ===============================\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    # ===============================\n",
    "    # MODEL PARAMETERS (UNCHANGED)\n",
    "    # ===============================\n",
    "    params = {\n",
    "        \"iterations\": 576,\n",
    "        \"learning_rate\": 0.07984126706847643,\n",
    "        \"depth\": 8,\n",
    "        \"l2_leaf_reg\": 1.66,\n",
    "        \"min_data_in_leaf\": 87,\n",
    "        \"class_weights\": class_weight_dict,\n",
    "        \"cat_features\": cat_feature_indices,\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": 100\n",
    "    }\n",
    "\n",
    "    # ===============================\n",
    "    # TRAIN MODEL\n",
    "    # ===============================\n",
    "    model = CatBoostClassifier(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # EVALUATION\n",
    "    # ===============================\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy_score(y_test, y_test_pred))\n",
    "    mlflow.log_metric(\"test_f1_macro\", f1_score(y_test, y_test_pred, average=\"macro\"))\n",
    "\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    mlflow.log_artifact(\"classification_report.txt\", \"evaluation\")\n",
    "\n",
    "    # ===============================\n",
    "    # SAVE PIPELINE (CRITICAL PART)\n",
    "    # ===============================\n",
    "    pipeline_model = ProfitPipelineModel(\n",
    "        model=model,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"profit_classifier_pipeline\",\n",
    "        python_model=pipeline_model,\n",
    "        registered_model_name=\"profit_classifier_pipeline\"\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # SAVE PREPROCESSING CONFIG\n",
    "    # ===============================\n",
    "    preprocessing_config = {\n",
    "        \"feature_names\": feature_names,\n",
    "        \"categorical_features\": cat_features,\n",
    "        \"categorical_indices\": cat_feature_indices,\n",
    "        \"class_labels\": [\"loss\", \"profit_low\", \"profit_high\"]\n",
    "    }\n",
    "\n",
    "    with open(\"preprocessing_config.json\", \"w\") as f:\n",
    "        json.dump(preprocessing_config, f, indent=2)\n",
    "\n",
    "    mlflow.log_artifact(\"preprocessing_config.json\", \"config\")\n",
    "\n",
    "    # ===============================\n",
    "    # REQUIREMENTS FOR DOCKER\n",
    "    # ===============================\n",
    "    requirements = \"\"\"mlflow==2.9.0\n",
    "catboost==1.2\n",
    "pandas==2.0.0\n",
    "numpy==1.24.0\n",
    "scikit-learn==1.3.0\n",
    "\"\"\"\n",
    "\n",
    "    with open(\"requirements.txt\", \"w\") as f:\n",
    "        f.write(requirements)\n",
    "\n",
    "    mlflow.log_artifact(\"requirements.txt\", \"docker\")\n",
    "\n",
    "    print(\" PIPELINE MODEL SAVED & REGISTERED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74111612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns:\n",
      "['product_id', 'product_name', 'category', 'brand', 'store_id', 'store_name', 'store_location', 'base_price', 'discount_rate', 'promotion_type', 'day_of_year', 'month', 'day_of_week', 'season', 'is_holiday', 'avg_units_sold_30d', 'avg_customers_30d', 'profit_margin', 'profit_class', 'profit_class_quantile', 'profit_class_3', 'profit_class_correct']\n",
      "\n",
      "Columns with 'profit' in name:\n",
      "['profit_margin', 'profit_class', 'profit_class_quantile', 'profit_class_3', 'profit_class_correct']\n",
      "\n",
      "Columns in X:\n",
      "['product_id', 'category', 'brand', 'store_id', 'store_location', 'base_price', 'discount_rate', 'promotion_type', 'day_of_year', 'month', 'day_of_week', 'season', 'is_holiday', 'avg_units_sold_30d', 'avg_customers_30d', 'profit_class_correct']\n",
      "\n",
      "Object columns in X:\n",
      "['product_id', 'category', 'brand', 'store_id', 'store_location', 'promotion_type', 'day_of_week', 'season', 'profit_class_correct']\n"
     ]
    }
   ],
   "source": [
    "# Before creating X, check for any profit-related columns\n",
    "print(\"All columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nColumns with 'profit' in name:\")\n",
    "profit_cols = [col for col in df.columns if 'profit' in col.lower()]\n",
    "print(profit_cols)\n",
    "\n",
    "# Check what's in X after dropping\n",
    "print(\"\\nColumns in X:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Check for any object/string columns that shouldn't be there\n",
    "print(\"\\nObject columns in X:\")\n",
    "print(X.select_dtypes(include='object').columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
